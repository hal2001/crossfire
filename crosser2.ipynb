{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Deconv2D\n",
    "from keras.layers import merge\n",
    "from keras.models import Model\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder2D(object):\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100): # size of minibatch\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.generator = None\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "class VariationalAutoencoder2D(Autoencoder2D):\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0):  # This is the stddev for our normal-dist sampling of the latent vector):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # Custom loss function for VAE. More VAE voodoo\n",
    "        # FC: NOTE: binary_crossentropy expects a batch_size by dim\n",
    "        # FC: for x and x_decoded_mean, so we MUST flatten these!\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        xent_loss = self.img_rows * self.img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        # Kullbackâ€“Leibler divergence. so many questions about this one single line\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "\n",
    "class Crossfire_MNIST_0(VariationalAutoencoder2D):\n",
    "    \"\"\" Covolutional VAE with a \"crossfire\" component - a classifier is bolted onto the end of the encoder\n",
    "    and loss function can be parameterized to function from classifier loss instead of autoencoder loss.\n",
    "    Ideally, the model starts in pure autoencoder mode to learn features, then as loss flattens out, the\n",
    "    network starts weighing classifier loss progressively greater \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std)\n",
    "\n",
    "\n",
    "\n",
    "        # theano vs tensorflow ordering of size parameters\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            self.original_img_size = (self.img_chns, self.img_rows, self.img_cols)\n",
    "        else:\n",
    "            self.original_img_size = (self.img_rows, self.img_cols, self.img_chns)\n",
    "\n",
    "\n",
    "        # experimental\n",
    "        def_conv = {'border_mode': 'same', 'activation': 'relu'}\n",
    "\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        x_in = Input(batch_shape=(batch_size,) + self.original_img_size, name='main_input')\n",
    "        conv_1 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu')(x_in)\n",
    "        conv_2 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                        subsample=(2, 2))(conv_1)\n",
    "        stack = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                       name='stack_base')(conv_2)\n",
    "\n",
    "        # I call this structure the \"stack\". By stacking convo layers w/ BN and dropout, the performance\n",
    "        # of the network increases dramatically. For MNIST, I like n_stacks=3.\n",
    "        # Presumably, the deepness allows for greater richness of filters to emerge\n",
    "        for i in range(n_stacks):\n",
    "            stack = BatchNormalization()(stack)\n",
    "            stack = Dropout(dropout_p)(stack)\n",
    "            stack = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                           name='stack_{}'.format(i), subsample=(1, 1))(stack)\n",
    "\n",
    "        stack = BatchNormalization()(stack)\n",
    "        conv_4 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu')(stack)\n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(conv_4)\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        zed = Dense(latent_dim)(hidden_1)\n",
    "        alpha = Lambda(lambda x: x[:,0:1], name='AlphaKeyVar')(zed) # output_shape=(1,) + input_shape[2:]\n",
    "        beta = Lambda(lambda x: x[:,1:], name='BetaResidualVar')(zed) # output_shape=(1,) + input_shape[2:]\n",
    "        z_mean = merge([alpha, beta], mode='concat')\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "        \n",
    "#         zed = Lambda(lambda x: x[:,0,:,:], output_shape=(1,) + input_shape[2:])(z_mean)\n",
    "#         zed = z_mean[:, 0]\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "        self.zed = zed\n",
    "\n",
    "        # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "        # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "        # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "\n",
    "        # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "        # The 'classer' is a subnet after the latent vector, which will drive the distribution in order to\n",
    "        # (hopefully) provide better generalization in classification\n",
    "        # Note: in the original Crossfile I attach this layer to z_mean, rather than z, for reasons I cannot recall\n",
    "        # I suspect this is because for classification, we do not care about the variance, just the mean of the vec\n",
    "        # In this setup, we go straight to one-hot\n",
    "        # Original uses normal init. Could try glorot or he_normal\n",
    "        # todo: test behavior of attachment point of the classer, different inits\n",
    "        classer_base = Dense(n_classes, init='normal', activation='softmax', name='classer_output')(self.z_mean)\n",
    "        #         classer_base = Dense(n_classes, init='normal', activation='softmax', name='classer_output')(z)\n",
    "\n",
    "        batch_size_dec = batch_size\n",
    "\n",
    "        # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "        # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filters * 14 * 14, activation='relu')\n",
    "\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size_dec, n_filters, 14, 14)\n",
    "        else:\n",
    "            output_shape = (batch_size_dec, 14, 14, n_filters)\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])  # FC's, I don't understand why this is here\n",
    "\n",
    "        # FC uses Deconv, but another example uses UpSample layers. See Keras Api: Deconvolution2D\n",
    "        decoder_deconv_1 = Deconv2D(n_filters, px_conv, px_conv, output_shape,\n",
    "                                    border_mode='same', activation='relu')\n",
    "        decoder_deconv_2 = Deconv2D(n_filters, px_conv, px_conv, output_shape,\n",
    "                                    border_mode='same', activation='relu')\n",
    "\n",
    "        # Some more reshaping, presumably I need to modify this in order to use different shapes\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size_dec, n_filters, 29, 29)\n",
    "        else:\n",
    "            output_shape = (batch_size_dec, 29, 29, n_filters)\n",
    "\n",
    "        # more FC voodoo\n",
    "        decoder_deconv_3_upsamp = Deconv2D(n_filters, 2, 2, output_shape, border_mode='valid', subsample=(2, 2),\n",
    "                                           activation='relu')\n",
    "        decoder_mean_squash = Conv2D(self.img_chns, 2, 2, border_mode='valid', activation='sigmoid', name='main_output')\n",
    "\n",
    "        # Now, piecemeal the encoder together. IDK why this is done this manner, and not functional like the\n",
    "        # encoder half. presumably, this is so we can inspect the output at each point\n",
    "        hid_decoded = decoder_hidden(z)\n",
    "        up_decoded = decoder_upsample(hid_decoded)\n",
    "        reshape_decoded = decoder_reshape(up_decoded)\n",
    "        deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "        deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "        x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "        x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "        # FC: build a digit generator that can sample from the learned distribution\n",
    "        # todo: (un)roll this\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        _hid_decoded = decoder_hidden(decoder_input)\n",
    "        _up_decoded = decoder_upsample(_hid_decoded)\n",
    "        _reshape_decoded = decoder_reshape(_up_decoded)\n",
    "        _deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "        _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "        _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "        _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "\n",
    "        # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "        # Primary model - VAE\n",
    "        self.model = Model(x_in, x_decoded_mean_squash)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        \n",
    "        self.zedmodel = Model(x_in, zed)\n",
    "        self.zedmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # Crossfire network\n",
    "        self.classifier = Model(x_in, classer_base)\n",
    "        self.classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        # Ok, now comes the tricky part. See these references:\n",
    "        # https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\n",
    "        # I believe the names have to match the layer names, but are otherwise arbitrary\n",
    "        self.crossmodel = Model(input=x_in, output=[x_decoded_mean_squash, classer_base])\n",
    "        self.crossmodel.compile(optimizer='rmsprop',\n",
    "                                loss={'main_output': self.vae_loss, 'classer_output': 'categorical_crossentropy'},\n",
    "                                loss_weights={'main_output': 1.0, 'classer_output': 5.0})\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x_in, self.z_mean)\n",
    "        # reconstruct the digit pictures from latent space\n",
    "        self.generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "\n",
    "\n",
    "\n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "                       validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks,\n",
    "                                                validation_split,\n",
    "                                                validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    \"\"\"\n",
    "    Base class for all-purpose autoencoder. VAE, CNN-AE, etc will be built off of this.\n",
    "\n",
    "    Input -> Encoder -> Z Latent Vector -> Decoder -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100, # size of minibatch\n",
    "                 compile_decoder=False # create the decoder. Not necessary for every use case\n",
    "                 ):\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.compile_decoder = compile_decoder\n",
    "        assert K_backend.image_dim_ordering() == 'tf', 'Cannot support Theano ordering! Use TF ordering! #tensorflowmasterrace'\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        # self.data_shape = input_shape[1:] # Shape of a single sample\n",
    "        if len(input_shape) == 4:\n",
    "            self.img_rows, self.img_cols, self.img_stacks, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        elif len(input_shape) == 1:\n",
    "            self.img_rows = input_shape[0]  # todo: test this\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "    def rollup_decoder(self, z, z_input, layers_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        last_ae = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        return last_ae, last_dc\n",
    "\n",
    "\n",
    "class VAE(Autoencoder):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0, # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=False\n",
    "                 ):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        # Necessary to instantiate this as instance variables such that they can be passed to the loss function (internally), since loss functions are\n",
    "        # all of the form lossfn(y_true, y_pred)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"\n",
    "        This is what makes the variational technique happen.\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        # We return z_mean + epsilon*sigma^2. Not sure why we use log var\n",
    "        # Basically, create a random variable vector from the distribution\n",
    "        # We are learning a distribution (mu, var) which represents the input\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        \"\"\"\n",
    "        Custom loss function for VAE. Uses Kullback-Leibler divergence.\n",
    "\n",
    "        Notes from fchollet: binary_crossentropy expects a shape (batch_size, dim) for x and x_decoded_mean,\n",
    "        so we MUST flatten these!\n",
    "        :param x:\n",
    "        :param x_decoded_mean:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        # Kullbackâ€“Leibler divergence. so many questions about this one single line\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "class VAE_MNIST_0(VAE):\n",
    "    \"\"\" Covolutional VAE for MNIST. Should work for other things, but untested \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=True,\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std,\n",
    "                         compile_decoder=compile_decoder)\n",
    "\n",
    "\n",
    "\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        print(self.input_shape)\n",
    "        x_in = Input( self.input_shape, name='main_input') # batch_shape=(batch_size,) +\n",
    "        conv_1 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu')(x_in)\n",
    "        conv_2 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                        subsample=(2, 2))(conv_1)\n",
    "        stack = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                       name='stack_base')(conv_2)\n",
    "\n",
    "        # I call this structure the \"stack\". By stacking convo layers w/ BN and dropout, the performance\n",
    "        # of the network increases dramatically. For MNIST, I like n_stacks=3.\n",
    "        # Presumably, the deepness allows for greater richness of filters to emerge\n",
    "        for i in range(n_stacks):\n",
    "            stack = BatchNormalization()(stack)\n",
    "            stack = Dropout(dropout_p)(stack)\n",
    "            stack = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                           name='stack_{}'.format(i), subsample=(1, 1))(stack)\n",
    "\n",
    "        stack = BatchNormalization()(stack)\n",
    "        conv_4 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu')(stack)\n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(conv_4)\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        \n",
    "        z_mean = Dense(latent_dim)(hidden_1)\n",
    "\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "        alpha = Lambda(lambda x: x[:,0:1], name='alpha_output')(z_mean) # output_shape=(1,) + input_shape[2:]\n",
    "        beta = Lambda(lambda x: x[:,1:], name='BetaResidualVar')(z_mean) # output_shape=(1,) + input_shape[2:]\n",
    "        \n",
    "        z_merge = merge([alpha, beta], mode='concat')\n",
    "\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "        self.z_merge = z_merge\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "        # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "        # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "\n",
    "        # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "        # The 'classer' is a subnet after the latent vector, which will drive the distribution in order to\n",
    "        # (hopefully) provide better generalization in classification\n",
    "        # Note: in the original Crossfile I attach this layer to z_mean, rather than z, for reasons I cannot recall\n",
    "        # I suspect this is because for classification, we do not care about the variance, just the mean of the vec\n",
    "        # In this setup, we go straight to one-hot\n",
    "        # Original uses normal init. Could try glorot or he_normal\n",
    "        # todo: test behavior of attachment point of the classer, different inits\n",
    "        classer_base = Dense(n_classes, init='normal', activation='softmax', name='classer_output')(self.z_mean)\n",
    "        #         classer_base = Dense(n_classes, init='normal', activation='softmax', name='classer_output')(z)\n",
    "\n",
    "        batch_size_dec = batch_size\n",
    "\n",
    "        # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "        # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filters * 14 * 14, activation='relu')\n",
    "\n",
    "\n",
    "        output_shape = (batch_size_dec, 14, 14, n_filters)\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])  # FC's, I don't understand why this is here\n",
    "\n",
    "        # FC uses Deconv, but another example uses UpSample layers. See Keras Api: Deconvolution2D\n",
    "        decoder_deconv_1 = Deconv2D(n_filters, px_conv, px_conv, output_shape,\n",
    "                                    border_mode='same', activation='relu')\n",
    "        decoder_deconv_2 = Deconv2D(n_filters, px_conv, px_conv, output_shape,\n",
    "                                    border_mode='same', activation='relu')\n",
    "\n",
    "        # Some more reshaping, presumably I need to modify this in order to use different shapes\n",
    "        output_shape = (batch_size_dec, 29, 29, n_filters)\n",
    "\n",
    "        # more FC voodoo\n",
    "        decoder_deconv_3_upsamp = Deconv2D(n_filters, 2, 2, output_shape, border_mode='valid', subsample=(2, 2),\n",
    "                                           activation='relu')\n",
    "        decoder_mean_squash = Conv2D(self.img_chns, 2, 2, border_mode='same', activation='sigmoid', name='main_output')\n",
    "\n",
    "        # Now, piecemeal the encoder together. IDK why this is done this manner, and not functional like the\n",
    "        # encoder half. presumably, this is so we can inspect the output at each point\n",
    "        # hid_decoded = decoder_hidden(z)\n",
    "        # up_decoded = decoder_upsample(hid_decoded)\n",
    "        # reshape_decoded = decoder_reshape(up_decoded)\n",
    "        # deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "        # deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "        # x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "        # x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "        hid_decoded = decoder_hidden\n",
    "        up_decoded = decoder_upsample\n",
    "        reshape_decoded = decoder_reshape\n",
    "        deconv_1_decoded = decoder_deconv_1\n",
    "        deconv_2_decoded = decoder_deconv_2\n",
    "        x_decoded_relu = decoder_deconv_3_upsamp\n",
    "        x_decoded_mean_squash = decoder_mean_squash\n",
    "\n",
    "        layers_list = [decoder_hidden, decoder_upsample, decoder_reshape, decoder_deconv_1, decoder_deconv_2,\n",
    "                       decoder_deconv_3_upsamp, decoder_mean_squash]\n",
    "\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "        # todo: better naming convention\n",
    "        ae, dc = self.rollup_decoder(z, decoder_input, layers_list)\n",
    "\n",
    "        if self.compile_decoder:\n",
    "            # FC: build a digit generator that can sample from the learned distribution\n",
    "            # todo: (un)roll this\n",
    "            _hid_decoded = decoder_hidden(decoder_input)\n",
    "            _up_decoded = decoder_upsample(_hid_decoded)\n",
    "            _reshape_decoded = decoder_reshape(_up_decoded)\n",
    "            _deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "            _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "            _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "            _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "\n",
    "        # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "        # Primary model - VAE\n",
    "        self.model = Model(x_in, ae)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        \n",
    "        self.zedmodel = Model(x_in, output=[ae, alpha])\n",
    "        self.zedmodel.compile(optimizer='adam',\n",
    "                              loss={'main_output': self.vae_loss, 'alpha_output': 'categorical_crossentropy'},\n",
    "                              loss_weights={'main_output': 1.0, 'alpha_output': 1.0},\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "        if False:\n",
    "\n",
    "            # Crossfire network\n",
    "            self.classifier = Model(x_in, classer_base)\n",
    "            self.classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            # Ok, now comes the tricky part. See these references:\n",
    "            # https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\n",
    "            # I believe the names have to match the layer names, but are otherwise arbitrary\n",
    "            self.crossmodel = Model(input=x_in, output=[x_decoded_mean_squash, classer_base])\n",
    "            self.crossmodel.compile(optimizer='rmsprop',\n",
    "                                    loss={'main_output': self.vae_loss, 'classer_output': 'categorical_crossentropy'},\n",
    "                                    loss_weights={'main_output': 1.0, 'classer_output': 5.0})\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x_in, self.z_mean)\n",
    "        if self.compile_decoder:\n",
    "            # reconstruct the digit pictures from latent space\n",
    "            self.decoder = Model(decoder_input, dc)\n",
    "            \n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "                       validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks,\n",
    "                                                validation_split,\n",
    "                                                validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:142: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:144: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", strides=(2, 2))`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:146: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), name=\"stack_base\", activation=\"relu\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:155: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"stack_0\", activation=\"relu\", strides=(1, 1))`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:155: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"stack_1\", activation=\"relu\", strides=(1, 1))`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:155: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"stack_2\", activation=\"relu\", strides=(1, 1))`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:158: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:172: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:195: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, name=\"classer_output\", activation=\"softmax\", kernel_initializer=\"normal\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:212: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:214: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:221: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (2, 2), activation=\"relu\", padding=\"valid\", strides=(2, 2))`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:222: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (2, 2), name=\"main_output\", activation=\"sigmoid\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:266: UserWarning: Update your `Model` call to the Keras 2 API: `Model(Tensor(\"ma..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "xf = VAE_MNIST_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'alpha_output_2/strided_slice:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xf.z_mean#[:, 0:]\n",
    "xf.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)               (None, 28, 28, 64)    640                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)               (None, 14, 14, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "stack_base (Conv2D)              (None, 14, 14, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNor (None, 14, 14, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 14, 14, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "stack_0 (Conv2D)                 (None, 14, 14, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNor (None, 14, 14, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 14, 14, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "stack_1 (Conv2D)                 (None, 14, 14, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNor (None, 14, 14, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)             (None, 14, 14, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "stack_2 (Conv2D)                 (None, 14, 14, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNor (None, 14, 14, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)               (None, 14, 14, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 12544)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "intermezzo (Dense)               (None, 256)           3211520                                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_59 (Dense)                 (None, 2)             514                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_60 (Dense)                 (None, 2)             514                                          \n",
      "____________________________________________________________________________________________________\n",
      "latent_z (Lambda)                (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_61 (Dense)                 (None, 256)           768                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_62 (Dense)                 (None, 12544)         3223808                                      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)              (None, 14, 14, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DTrans (None, 14, 14, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DTrans (None, 14, 14, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DTrans (None, 28, 28, 64)    16448                                        \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Conv2D)             (None, 28, 28, 1)     257                                          \n",
      "====================================================================================================\n",
      "Total params: 6,750,917.0\n",
      "Trainable params: 6,750,405.0\n",
      "Non-trainable params: 512.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xf.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# train the VAE on MNIST digits\n",
    "from keras.datasets import mnist\n",
    "original_img_size = (28,28,1)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "batch_size = 100\n",
    "print(original_img_size)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (10000, 10)\n",
      "x_train.shape: (60000, 28, 28, 1)\n",
      "x_train.shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# one hot encode outputs\n",
    "y_train_oh = np_utils.to_categorical(y_train)\n",
    "y_test_oh = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test_oh.shape[1]\n",
    "print(y_train.shape, y_test_oh.shape)\n",
    "print('x_train.shape:', x_train.shape)\n",
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:  (60000, 28, 28, 1) (10000, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2cdd99eb514270b3cbce7425b12c33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482e29364ce24cffa1c52229ef0d1386"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-5e7a38cbaca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         callbacks=[TQDMNotebookCallback()])\n\u001b[0m",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print('shapes: ', x_train.shape, x_test.shape)\n",
    "xf.model.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test),\n",
    "        verbose=False,\n",
    "        callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
