{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Deconv2D\n",
    "from keras.layers import merge\n",
    "from keras.models import Model\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder2D(object):\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100): # size of minibatch\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.generator = None\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "class VariationalAutoencoder2D(Autoencoder2D):\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0):  # This is the stddev for our normal-dist sampling of the latent vector):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # Custom loss function for VAE. More VAE voodoo\n",
    "        # FC: NOTE: binary_crossentropy expects a batch_size by dim\n",
    "        # FC: for x and x_decoded_mean, so we MUST flatten these!\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        xent_loss = self.img_rows * self.img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        # Kullbackâ€“Leibler divergence. so many questions about this one single line\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "\n",
    "class Crossfire_MNIST_0(VariationalAutoencoder2D):\n",
    "    \"\"\" Covolutional VAE with a \"crossfire\" component - a classifier is bolted onto the end of the encoder\n",
    "    and loss function can be parameterized to function from classifier loss instead of autoencoder loss.\n",
    "    Ideally, the model starts in pure autoencoder mode to learn features, then as loss flattens out, the\n",
    "    network starts weighing classifier loss progressively greater \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std)\n",
    "\n",
    "\n",
    "\n",
    "        # theano vs tensorflow ordering of size parameters\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            self.original_img_size = (self.img_chns, self.img_rows, self.img_cols)\n",
    "        else:\n",
    "            self.original_img_size = (self.img_rows, self.img_cols, self.img_chns)\n",
    "\n",
    "\n",
    "        # experimental\n",
    "        def_conv = {'border_mode': 'same', 'activation': 'relu'}\n",
    "\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        x_in = Input(batch_shape=(batch_size,) + self.original_img_size, name='main_input')\n",
    "        conv_1 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu')(x_in)\n",
    "        conv_2 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                        subsample=(2, 2))(conv_1)\n",
    "        stack = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                       name='stack_base')(conv_2)\n",
    "\n",
    "        # I call this structure the \"stack\". By stacking convo layers w/ BN and dropout, the performance\n",
    "        # of the network increases dramatically. For MNIST, I like n_stacks=3.\n",
    "        # Presumably, the deepness allows for greater richness of filters to emerge\n",
    "        for i in range(n_stacks):\n",
    "            stack = BatchNormalization()(stack)\n",
    "            stack = Dropout(dropout_p)(stack)\n",
    "            stack = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu',\n",
    "                           name='stack_{}'.format(i), subsample=(1, 1))(stack)\n",
    "\n",
    "        stack = BatchNormalization()(stack)\n",
    "        conv_4 = Conv2D(n_filters, px_conv, px_conv, border_mode='same', activation='relu')(stack)\n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(conv_4)\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        zed = Dense(latent_dim)(hidden_1)\n",
    "        alpha = Lambda(lambda x: x[:,0:1], name='AlphaKeyVar')(zed) # output_shape=(1,) + input_shape[2:]\n",
    "        beta = Lambda(lambda x: x[:,1:], name='BetaResidualVar')(zed) # output_shape=(1,) + input_shape[2:]\n",
    "        z_mean = merge([alpha, beta], mode='concat')\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "        \n",
    "#         zed = Lambda(lambda x: x[:,0,:,:], output_shape=(1,) + input_shape[2:])(z_mean)\n",
    "#         zed = z_mean[:, 0]\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "        self.zed = zed\n",
    "\n",
    "        # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "        # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "        # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "\n",
    "        # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "        # The 'classer' is a subnet after the latent vector, which will drive the distribution in order to\n",
    "        # (hopefully) provide better generalization in classification\n",
    "        # Note: in the original Crossfile I attach this layer to z_mean, rather than z, for reasons I cannot recall\n",
    "        # I suspect this is because for classification, we do not care about the variance, just the mean of the vec\n",
    "        # In this setup, we go straight to one-hot\n",
    "        # Original uses normal init. Could try glorot or he_normal\n",
    "        # todo: test behavior of attachment point of the classer, different inits\n",
    "        classer_base = Dense(n_classes, init='normal', activation='softmax', name='classer_output')(self.z_mean)\n",
    "        #         classer_base = Dense(n_classes, init='normal', activation='softmax', name='classer_output')(z)\n",
    "\n",
    "        batch_size_dec = batch_size\n",
    "\n",
    "        # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "        # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filters * 14 * 14, activation='relu')\n",
    "\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size_dec, n_filters, 14, 14)\n",
    "        else:\n",
    "            output_shape = (batch_size_dec, 14, 14, n_filters)\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])  # FC's, I don't understand why this is here\n",
    "\n",
    "        # FC uses Deconv, but another example uses UpSample layers. See Keras Api: Deconvolution2D\n",
    "        decoder_deconv_1 = Deconv2D(n_filters, px_conv, px_conv, output_shape,\n",
    "                                    border_mode='same', activation='relu')\n",
    "        decoder_deconv_2 = Deconv2D(n_filters, px_conv, px_conv, output_shape,\n",
    "                                    border_mode='same', activation='relu')\n",
    "\n",
    "        # Some more reshaping, presumably I need to modify this in order to use different shapes\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size_dec, n_filters, 29, 29)\n",
    "        else:\n",
    "            output_shape = (batch_size_dec, 29, 29, n_filters)\n",
    "\n",
    "        # more FC voodoo\n",
    "        decoder_deconv_3_upsamp = Deconv2D(n_filters, 2, 2, output_shape, border_mode='valid', subsample=(2, 2),\n",
    "                                           activation='relu')\n",
    "        decoder_mean_squash = Conv2D(self.img_chns, 2, 2, border_mode='valid', activation='sigmoid', name='main_output')\n",
    "\n",
    "        # Now, piecemeal the encoder together. IDK why this is done this manner, and not functional like the\n",
    "        # encoder half. presumably, this is so we can inspect the output at each point\n",
    "        hid_decoded = decoder_hidden(z)\n",
    "        up_decoded = decoder_upsample(hid_decoded)\n",
    "        reshape_decoded = decoder_reshape(up_decoded)\n",
    "        deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "        deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "        x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "        x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "        # FC: build a digit generator that can sample from the learned distribution\n",
    "        # todo: (un)roll this\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        _hid_decoded = decoder_hidden(decoder_input)\n",
    "        _up_decoded = decoder_upsample(_hid_decoded)\n",
    "        _reshape_decoded = decoder_reshape(_up_decoded)\n",
    "        _deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "        _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "        _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "        _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "\n",
    "        # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "        # Primary model - VAE\n",
    "        self.model = Model(x_in, x_decoded_mean_squash)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        \n",
    "        self.zedmodel = Model(x_in, zed)\n",
    "        self.zedmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # Crossfire network\n",
    "        self.classifier = Model(x_in, classer_base)\n",
    "        self.classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        # Ok, now comes the tricky part. See these references:\n",
    "        # https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\n",
    "        # I believe the names have to match the layer names, but are otherwise arbitrary\n",
    "        self.crossmodel = Model(input=x_in, output=[x_decoded_mean_squash, classer_base])\n",
    "        self.crossmodel.compile(optimizer='rmsprop',\n",
    "                                loss={'main_output': self.vae_loss, 'classer_output': 'categorical_crossentropy'},\n",
    "                                loss_weights={'main_output': 1.0, 'classer_output': 5.0})\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x_in, self.z_mean)\n",
    "        # reconstruct the digit pictures from latent space\n",
    "        self.generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "\n",
    "\n",
    "\n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "                       validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks,\n",
    "                                                validation_split,\n",
    "                                                validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:97: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", name=\"stack_base\", activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:110: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), name=\"stack_0\", activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:110: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), name=\"stack_1\", activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:110: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(1, 1), name=\"stack_2\", activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:123: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:148: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, kernel_initializer=\"normal\", name=\"classer_output\", activation=\"softmax\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:167: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:169: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:179: UserWarning: Update your `Conv2DTranspose` call to the Keras 2 API: `Conv2DTranspose(64, (2, 2), padding=\"valid\", strides=(2, 2), activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (2, 2), padding=\"valid\", name=\"main_output\", activation=\"sigmoid\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:217: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"ma..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "xf = Crossfire_MNIST_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_48/BiasAdd:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xf.z_mean#[:, 0:]\n",
    "xf.zed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (100, 28, 28, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)               (100, 28, 28, 64)     640                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)               (100, 14, 14, 64)     36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "stack_base (Conv2D)              (100, 14, 14, 64)     36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNor (100, 14, 14, 64)     256                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)             (100, 14, 14, 64)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "stack_0 (Conv2D)                 (100, 14, 14, 64)     36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNor (100, 14, 14, 64)     256                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)             (100, 14, 14, 64)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "stack_1 (Conv2D)                 (100, 14, 14, 64)     36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNor (100, 14, 14, 64)     256                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)             (100, 14, 14, 64)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "stack_2 (Conv2D)                 (100, 14, 14, 64)     36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNor (100, 14, 14, 64)     256                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)               (100, 14, 14, 64)     36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (100, 12544)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "intermezzo (Dense)               (100, 256)            3211520                                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_48 (Dense)                 (100, 2)              514                                          \n",
      "____________________________________________________________________________________________________\n",
      "AlphaKeyVar (Lambda)             (100, 1)              0                                            \n",
      "____________________________________________________________________________________________________\n",
      "BetaResidualVar (Lambda)         (100, 1)              0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (100, 2)              0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_49 (Dense)                 (100, 2)              514                                          \n",
      "____________________________________________________________________________________________________\n",
      "latent_z (Lambda)                (100, 2)              0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_50 (Dense)                 multiple              768                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_51 (Dense)                 multiple              3223808                                      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)              multiple              0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DTrans multiple              36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DTrans multiple              36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DTrans multiple              16448                                        \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Conv2D)             multiple              257                                          \n",
      "====================================================================================================\n",
      "Total params: 6,750,917.0\n",
      "Trainable params: 6,750,405.0\n",
      "Non-trainable params: 512.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xf.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
